{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3KwdpDiYPfR4YSIM/lsIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devanshi-Sharma12/HATESPEECHDETECTION/blob/main/hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwA6QPrq_QdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn. feature_extraction. text import CountVectorizer\n",
        "from sklearn. model_selection import train_test_split\n",
        "from sklearn. tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "#nltk. download('stopwords')\n",
        "from nltk. corpus import stopwords\n",
        "stopword=set(stopwords.words('english'))\n",
        "stemmer = nltk. SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nn0EjWdA_eqX",
        "outputId": "11f0989d-6218-41d9-951a-c6de61dd6a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-20233d9a96fe>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#nltk. download('stopwords')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstopword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1sOKqko_3YQ",
        "outputId": "4781506f-d3cc-4e63-eaf0-8fc77cdd737e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "# Uncomment the line below to download stopwords if not already downloaded\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopword = set(stopwords.words('english'))\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "\n",
        "# Now you can proceed with using the stopwords and stemmer objects in your script\n"
      ],
      "metadata": {
        "id": "0cXndiHu_5Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "9shBJ0_F_-c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LojUw7BhAEWl",
        "outputId": "6d21668a-61b1-438d-f570-e4688ce7baa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8rO6v22AIVl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RN5KmPN2Aj5S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Double check this path for typos and existence\n",
        "target_directory = 'C:\\\\Users\\\\DELL\\\\Documents'\n",
        "\n",
        "if os.path.exists(target_directory):\n",
        "    os.chdir(target_directory)\n",
        "    print(\"Successfully changed directory to:\", target_directory)\n",
        "else:\n",
        "    print(\"Directory not found:\", target_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0SX9JaOBMa3",
        "outputId": "cd8b01c0-66fc-405a-8c98-56d160a6c957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory not found: C:\\Users\\DELL\\Documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Example: Change directory to a folder in Google Drive\n",
        "os.chdir('/content/drive/My Drive/Documents')\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RYqmHltLBX64",
        "outputId": "4ef2e46a-2d01-4a29-9def-993c5a07a99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Documents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d2596a0cdb40>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Example: Change directory to a folder in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Documents'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current working directory: {os.getcwd()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Documents'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List contents of My Drive to verify structure\n",
        "drive_path = '/content/drive/My Drive'\n",
        "contents = os.listdir(drive_path)\n",
        "print(contents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk23HkyYEhv_",
        "outputId": "e4a9d1ce-ab05-4b1d-8fd9-e47323167d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BharatScanner-CloudSync', 'vcards_20211201_190926.vcf', 'IMG_20180214_145202585.jpg', 'Polish_20211019_172238584.jpg', 'Polish_20211019_172058776.jpg', 'Polish_20211018_162908987.jpg', 'Polish_20211003_175917315.jpg', 'Polish_20211001_141138818.jpg', 'Polish_20210925_180749697.jpg', 'Polish_20210616_124021731.jpg', 'IMG_20210726_192148_907.jpg', 'Polish_20210616_123743042.jpg', 'Polish_20210616_121926197.jpg', 'Polish_20210605_202044669.jpg', 'IMG_20210503_182407_024.jpg', 'Polish_20210503_170700489.jpg', 'Polish_20210426_181122259.jpg', 'Polish_20210426_174330853.jpg', 'Polish_20210425_222529908.jpg', 'Polish_20210412_232403356.jpg', 'Polish_20210306_172908689.jpg', 'Polish_20210220_224750795.jpg', 'Polish_20210217_151236900.jpg', 'Polish_20210217_150211615.jpg', 'Polish_20210115_230244119.jpg', 'Polish_20210115_230012231.jpg', 'Polish_20210113_185756795.jpg', 'IMG_20210413_023400_220.jpg', 'Polish_20201230_124346146.jpg', 'Polish_20201227_132542505.jpg', 'Polish_20201225_234650120.jpg', 'Polish_20201225_234622547.jpg', 'Polish_20201127_181512437.jpg', 'IMG_20201124_000145_543.JPG', 'IMG_20201124_001506_018.jpg', 'Polish_20201105_174332103.jpg', 'Polish_20201021_204832591.jpg', 'Polish_20201021_171024088.jpg', 'bfd1272cac1deabe1fc51ff8e0bdec13.jpg', 'IMG-20200910-WA0028.jpg', 'VID-20211130-WA0005.mp4', 'VID-20211130-WA0004.mp4', 'VID-20211130-WA0001.mp4', 'VID-20211125-WA0009.mp4', 'VID-20211124-WA0012.mp4', 'VID-20211123-WA0001.mp4', 'VID-20211122-WA0046.mp4', 'VID-20211122-WA0042.mp4', 'VID-20211121-WA0029.mp4', 'VID-20211120-WA0001.mp4', 'VID-20211119-WA0007.mp4', 'VID-20211114-WA0009.mp4', 'VID-20211112-WA0031.mp4', 'VID-20211103-WA0005.mp4', 'VID-20211103-WA0004.mp4', 'VID-20211012-WA0031.mp4', 'VID-20211004-WA0036.mp4', 'VID-20211004-WA0030.mp4', 'VID-20211004-WA0019.mp4', 'VID-20211004-WA0015.mp4', 'VID-20210924-WA0004.mp4', 'VID-20210924-WA0000.mp4', 'VID-20210920-WA0002.mp4', 'VID-20210905-WA0002.mp4', 'VID-20210818-WA0001.mp4', 'VID-20210816-WA0019.mp4', 'VID-20210816-WA0017.mp4', 'VID-20210731-WA0027.mp4', 'VID-20210731-WA0026.mp4', 'VID-20210731-WA0022.mp4', 'VID-20210730-WA0000.mp4', 'VID-20210722-WA0012.mp4', 'VID-20210629-WA0000.mp4', 'VID-20210624-WA0002.mp4', 'VID-20210623-WA0008.mp4', 'VID-20210520-WA0000.mp4', 'VID-20210514-WA0010.mp4', 'VID-20210422-WA0001.mp4', 'VID-20210421-WA0009.mp4', 'VID-20210329-WA0003.mp4', 'VID-20210322-WA0000.mp4', 'VID-20210320-WA0003.mp4', 'VID-20210320-WA0002.mp4', 'VID-20210320-WA0001.mp4', 'VID-20210319-WA0021.mp4', 'VID-20210319-WA0020.mp4', 'VID-20210319-WA0003.mp4', 'VID-20210315-WA0000.mp4', 'VID-20210112-WA0017.mp4', 'VID-20210112-WA0016.mp4', 'VID-20201223-WA0000.mp4', 'VID-20201208-WA0004.mp4', 'VID-20201208-WA0003.mp4', 'VID-20200816-WA0012.mp4', 'VID-20201114-WA0003.mp4', 'VID-20201005-WA0006.mp4', 'VID-20200929-WA0004.mp4', 'VID-20200910-WA0051.mp4', 'y2mate.com - Kina Chir  The PropheC  Black Screen Status _1080pFHR.mp4', 'y2mate.com - soulmate song full screen status soulmate whatsapp status  tu hi mera soulmate forever wala pyar_1080p.mp4', 'y2mate.com - Tujh Mein Rab Dikhta Hai  Female Version  Rab Ne Bana Di Jodi SRK Anushka Sharma Shreya Ghoshal_1080p.mp4', '12th Marksheet.jpeg', '10th Marksheet.jpg', 'Transfer Certificate.jpg', 'y2mate.com - Scene Filmfare Award Best Scene of The Year 2008  Rab Ne Bana Di Jodi  Shah Rukh Khan_480p.mp4', 'y2mate.com - Jab Kahin Pe Kuch Nahi Bhi Nahi Tha  WhatsAppStatus  Kun Faya Kun _1080pFHR.mp4', 'y2mate.com - Friend  funny WhatsApp status amit nanama_360p.mp4', 'Marksheet.jpeg', 'IMG_20211102_172034_730.jpg', 'TC.jpeg', 'Cartificate.jpeg', 'y2mate.com - Do you need any help_1080p.mp4', 'y2mate.com - Jannat Ve Status  Jannat Ve Darshan Raval Status  Darshan Raval New Song  Jannat Ve Song Status_1080pFHR.mp4', '234296111_1298093983982489_1953364928686488022_n.mp4', 'y2mate.com - Billie eilish  WhatsApp status sad_480p.mp4', 'y2mate.com - Tujhe Kitna Chahne Lage Hum  WhatsAppStatus  LoveStatus  Kabir Singh   Arijit Singh_1080p.mp4', 'y2mate.com - LOVELY  BILLIE EILISH I JUSTIN DEGRYSE I WHATSAPP STATUS I ENGSUB_1080p.mp4', 'y2mate.com - tujhe mein rab dikhta hai cover  janani sings  i hate everyone but  female cover_1080p.mp4', 'y2mate.com - IS DUNIYA KI IS BHEED ME KAHIN KHO NA JAU MAI  EK TARFA SONG WHASTAPP STATUS AND RINGTONE_360p.mp4', 'y2mate.com - Saurabh Raj Jain status for Mahabharat show #digitalsunny_360p.mp4', 'y2mate.com - Love Whatsapp Status Video !! Instagram Love Status !! Instagram Status_v720P.mp4', 'IMG_20211128_123753.jpg', 'IMG_20211108_213719.jpg', 'IMG_20210818_190653.jpg', 'IMG_20210715_113500.jpg', 'BeautyPlusMe_20210616144253_save.jpg', 'IMG_20210604_111744.jpg', 'IMG_20210304_163942.jpg', 'BeautyPlusMe_20201028235727_save.jpg', 'IMG_20201004_205400.jpg', 'Polish_20210525_195948053.jpg', 'Polish_20210408_122347969.jpg', 'VID-20211126-WA0000.mp4', 'VID-20211121-WA0030.mp4', 'VID-20210816-WA0034.mp4', 'VID-20210319-WA0019.mp4', 'IMG_20201109_140732.jpg', 'algorithm-manual.pdf', 'UC-5e919ff5-41ae-4165-bcd6-685662b210c3.jpg', 'IsometricDevanshiSharmaA8.pdf', 'LateralsurfaceA8DevanshiSharma.pdf', 'ProjectionsofSolidsA8DevanshiSharma.pdf', 'DevanshiSharmaA8Isometric.pdf', 'Screenshot_2022-04-08-09-44-43-346_com.miui.gallery.jpg', \" 'DevanshiSharma2018780(G1).docx'\", ' DevanshiSharma2018780(G1).docx', 'Cloud Deployment Model | Rabin Khatri - Academia.edu', 'Presented by Howard Ong (7).pdf', 'Cloud computing deployment model.pdf', 'Chatbot presentation .pdf', 'Cloud deployment model.pdf', 'Chatbot project .pdf', \"Devanshi.docx' with you from WPS Office\", 'DevanshiSharma_2018780.docx', 'Share DevanshiSharma_2018780.docx', 'Adobe Scan 11-Dec-2022 (1).pdf', 'blank (1).jpg', 'blank.jpg', 'Adobe Scan 11-Dec-2022.pdf', \"I am sharing 'TERM WORK.docx' with you from WPS Office\", 'Share devanshi_cpp.docx', 'FRONT PAGE LOGO_DEVANSHI.docx', 'Share RMP (FINAL).docx', 'certificate.pdf', 'FiniteautomataDevanshiH20.pdf', 'HTML.pdf', 'Adobe Certificates (Beginner) (1).pdf', 'AI.pdf', 'ANDROID.pdf', 'devanshi20.pdf', 'GeuAdmitCard (4) (2).pdf', 'Funny tom and jerry little duck talking', 'GeuAdmitCard (4) (1).pdf', 'GeuAdmitCard (4).pdf', 'Copy of CSE_MINI_PROJECT_Report_Template.docx', 'IMAGEPROCESSINGMINIPROJECT4 ', 'Imageprocessingminiproject', 'Devanshi Sharma OS (20).pdf', 'Devanshi Sharma_Marksheet.pdf', 'Devanshi Sharma_College ID.pdf', 'Devanshi Sharma_Aadhar card.pdf', 'Devanshi Sharma_Resume', 'aman', 'Aniket', 'Classroom', 'system.jpg', 'bs.jpg', 'Q3-7.png', 'Document from Devanshi Sharma (4).pdf', 'Document from Devanshi Sharma (3).pdf', 'Document from Devanshi Sharma (2).pdf', 'Document from Devanshi Sharma (1).pdf', 'Document from Devanshi Sharma.pdf', 'certificates', 'Colab Notebooks', 'diabetes.zip', 'project', 'Data_Extraction_and_NLP', 'blackassign0001.gdoc', 'TestAssignment', 'updated.pdf', 'mini project report on sign language detection.gdoc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "drive_path = '/content/drive/My Drive'\n",
        "documents_path = os.path.join(drive_path, 'Documents')\n",
        "\n",
        "# Create 'Documents' directory if it doesn't exist\n",
        "if not os.path.exists(documents_path):\n",
        "    os.makedirs(documents_path)\n",
        "    print(f\"Created '{documents_path}' directory.\")\n",
        "\n",
        "# Now try accessing or working with 'Documents'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UfgeQQkKwtx",
        "outputId": "9eec78dc-8a26-40e1-bc23-831db1dba0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created '/content/drive/My Drive/Documents' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zf2wIzVVK_S0",
        "outputId": "c74ee669-d2d0-469f-d0da-cbf4c857c3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "id": "BUvIj3V_K15H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d86f4d25-eb69-4c33-f297-0144ee14e6d8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the Documents folder in Google Drive\n",
        "drive_path = '/content/drive/My Drive'\n",
        "documents_path = os.path.join(drive_path, 'Documents')\n",
        "\n",
        "# Check if the Documents folder exists and create it if necessary\n",
        "if not os.path.exists(documents_path):\n",
        "    os.makedirs(documents_path)\n",
        "    print(f\"Created '{documents_path}' directory.\")\n",
        "\n",
        "# Change the working directory to the Documents folder\n",
        "os.chdir(documents_path)\n",
        "print(f\"Changed directory to '{documents_path}' successfully.\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_zfekcYL61L",
        "outputId": "a7f9d03c-dfc5-42bc-f65e-b271f1aba683"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Changed directory to '/content/drive/My Drive/Documents' successfully.\n",
            "Current working directory: /content/drive/My Drive/Documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q7du4d6_L_uw",
        "outputId": "3c863e2c-af49-40c4-ad80-d20e79ba1e5f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Documents'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd. read_csv(\"labeled_data.csv\")\n",
        "#To preview the data\n",
        "print(data. head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWRY6KjZMDgx",
        "outputId": "488885b5-b4f9-4428-cb89-50fd87acaf39"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"labels\"] = data[\"class\"]. map({0: \"Hate Speech\", 1: \"Offensive Speech\", 2: \"No Hate and Offensive Speech\"})\n",
        "data = data[[\"tweet\", \"labels\"]]\n",
        "print(data. head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftRdpTbYPVid",
        "outputId": "9117d55d-8353-4de4-93af-e8576d793546"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet  \\\n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
            "\n",
            "                         labels  \n",
            "0  No Hate and Offensive Speech  \n",
            "1              Offensive Speech  \n",
            "2              Offensive Speech  \n",
            "3              Offensive Speech  \n",
            "4              Offensive Speech  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "glNO-KJEPaXz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean (text):\n",
        " text = str (text). lower()\n",
        " text = re. sub('\\[.*?\\]', '', text)\n",
        " text = re. sub('https?://\\S+|www\\.\\S+', '', text)\n",
        " text = re. sub('<.*?>+', '', text)\n",
        " text = re. sub('[%s]' % re. escape(string. punctuation), '', text)\n",
        " text = re. sub('\\n', '', text)\n",
        " text = re. sub('\\w*\\d\\w*', '', text)\n",
        " text = [word for word in text.split(' ') if word not in stopword]\n",
        " text=\" \". join(text)\n",
        " text = [stemmer. stem(word) for word in text. split(' ')]\n",
        " text=\" \". join(text)\n",
        " return text\n",
        "data[\"tweet\"] = data[\"tweet\"]. apply(clean)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61S_eWxJPecO",
        "outputId": "6141e5e2-4536-4bbc-9d8a-2bece268534e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet  \\\n",
            "0   rt mayasolov woman shouldnt complain clean ho...   \n",
            "1   rt  boy dat coldtyga dwn bad cuffin dat hoe  ...   \n",
            "2   rt urkindofbrand dawg rt  ever fuck bitch sta...   \n",
            "3             rt cganderson vivabas look like tranni   \n",
            "4   rt shenikarobert shit hear might true might f...   \n",
            "\n",
            "                         labels  \n",
            "0  No Hate and Offensive Speech  \n",
            "1              Offensive Speech  \n",
            "2              Offensive Speech  \n",
            "3              Offensive Speech  \n",
            "4              Offensive Speech  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np. array(data[\"tweet\"])\n",
        "y = np. array(data[\"labels\"])\n",
        "cv = CountVectorizer()\n",
        "X = cv. fit_transform(x)\n",
        "# Splitting the Data\n",
        "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "JnOSpFD6Php9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model building\n",
        "model = DecisionTreeClassifier()\n",
        "#Training the model\n",
        "model. fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "woMsAwxEPlK6",
        "outputId": "75bf7204-61c9-4b9a-b557-f9dfbf1012ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model\n",
        "y_pred = model. predict (X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8HXE0DDPp3T",
        "outputId": "c853df4c-f84b-455d-a767-5c5047359247"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Offensive Speech', 'Offensive Speech', 'Offensive Speech', ...,\n",
              "       'Offensive Speech', 'Offensive Speech', 'Offensive Speech'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy Score of our model\n",
        "from sklearn. metrics import accuracy_score\n",
        "print (accuracy_score (y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeZnFt4CSIyn",
        "outputId": "ae58e2f9-7a12-4f4c-b49b-177e8e7e4f04"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.87687981415821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the outcome\n",
        "inp = \"You are too bad and I dont like your attitude\"\n",
        "inp = cv.transform([inp]).toarray()\n",
        "print(model.predict(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_03fSI4SRba",
        "outputId": "e977db12-b5ce-4215-ad78-62365783db52"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Offensive Speech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"It is really awesome\"\n",
        "inp = cv. transform([inp]). toarray()\n",
        "print(model. predict(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCR2fiusSZKT",
        "outputId": "e0898ff6-5c7f-42e4-93d0-8b84e33bb4bb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No Hate and Offensive Speech']\n"
          ]
        }
      ]
    }
  ]
}